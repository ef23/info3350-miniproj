{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming 6 continued: Visualization of Burrows' Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, sys, os\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import numpy, scipy.stats\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made some small edits to the metadata file, mostly to remove extra newlines and regularize author names. I saved a separate copy to avoid overwriting any changes you have made locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "###                                      note name change â†“ \n",
    "with open(\"../data/FederalistPapers/metadata_federalist_fixed.csv\", encoding=\"utf-8\") as reader:\n",
    "    csv_reader = csv.DictReader(reader)\n",
    "    for row in csv_reader:\n",
    "        ## convert string to int\n",
    "        row[\"Number\"] = int(row[\"Number\"])\n",
    "        \n",
    "        \n",
    "        row[\"Filename\"] = \"../data/FederalistPapers/federalist_{:02d}.txt\".format(row[\"Number\"])\n",
    "        if os.path.exists(row[\"Filename\"]):\n",
    "            documents.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in documents:\n",
    "    try:\n",
    "        with open(document[\"Filename\"], encoding=\"utf-8\") as reader:\n",
    "            print(document[\"Title\"])\n",
    "\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                lines.append(line.rstrip())\n",
    "\n",
    "            text = \" \".join(lines)\n",
    "            document[\"Spacy\"] = nlp(text)\n",
    "    except:\n",
    "        print(\"Problem with {}\".format(document[\"Number\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lengths = []\n",
    "\n",
    "for document in documents:\n",
    "    lengths = [len(sentence) for sentence in document[\"Spacy\"]]\n",
    "    sentence_lengths.append(lengths)\n",
    "\n",
    "author_sentence_lengths = {}\n",
    "for doc_id, document in enumerate(documents):\n",
    "    author = document[\"Author\"]\n",
    "    if not author in author_sentence_lengths:\n",
    "        author_sentence_lengths[ author ] = []\n",
    "    author_sentence_lengths[ author ].extend(sentence_lengths[doc_id])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counts = Counter()\n",
    "\n",
    "for document in documents:\n",
    "    doc_counter = Counter([token.text for token in document[\"Spacy\"]])\n",
    "    all_counts += doc_counter   \n",
    "    document[\"TokenCounts\"] = doc_counter\n",
    "\n",
    "all_counts.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words = 150\n",
    "top_words = [w for w, c in all_counts.most_common(num_top_words)]\n",
    "\n",
    "doc_word_counts = numpy.zeros( (len(documents), num_top_words) )\n",
    "\n",
    "for doc_id, document in enumerate(documents):\n",
    "    for word_id, word in enumerate(top_words):\n",
    "        doc_word_counts[doc_id,word_id] = document[\"TokenCounts\"][word]\n",
    "\n",
    "doc_word_counts[:5,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lengths = doc_word_counts.sum(axis=1)\n",
    "print(doc_lengths.shape)\n",
    "doc_word_probs = doc_word_counts / doc_lengths[:,numpy.newaxis]\n",
    "doc_word_probs[:5,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_means = doc_word_probs.mean(axis=0)\n",
    "word_sds = doc_word_probs.std(axis=0)\n",
    "\n",
    "doc_word_zscores = (doc_word_probs - word_means[numpy.newaxis,:]) / word_sds[numpy.newaxis,:]  ## subtract means, divide by std\n",
    "doc_word_zscores[:5,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part : Individual words\n",
    "\n",
    "These plots will show the relative scores of individual words most frequently used. Be as specific as you can in each written answer. Feel free to add any additional measurements if you find them useful in making your arguments.\n",
    "\n",
    "* Which words are used most differently between authors?\n",
    "* Do you think these differences are consistent?\n",
    "* Are the scores within a specific numeric range? Why or why not?\n",
    "* Was this result surprising? \n",
    "\n",
    "[Answer below]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word_id, word in enumerate(top_words[:10]):\n",
    "    madison_scores = doc_word_zscores[author_list == \"James Madison\", word_id]\n",
    "    hamilton_scores = doc_word_zscores[author_list == \"Alexander Hamilton\", word_id]\n",
    "    \n",
    "    pyplot.xlim(-2.5, 2.5)\n",
    "    pyplot.title(\"\\\"{}\\\" M:{:.2f} / H:{:.2f}\".format(word, numpy.mean(madison_scores), numpy.mean(hamilton_scores)))\n",
    "    pyplot.hist([madison_scores, hamilton_scores], 10, histtype='bar')\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part : Multiple words\n",
    "\n",
    "These plots show heat maps for all words.\n",
    "\n",
    "* Does this view change your perspective on the words you looked at in the previous section? \n",
    "* Why is John Jay so distinctive?\n",
    "* Are authors consistent in their relative use of words over the different essays?\n",
    "* How do the most frequent words compare to the less frequent words?\n",
    "* How many words would you want to count before you would feel confident distinguishing any pair of these three authors?\n",
    "\n",
    "[Answer below]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_list = numpy.array([doc[\"Author\"] for doc in documents])\n",
    "\n",
    "for author in [\"John Jay\", \"James Madison\", \"Alexander Hamilton or James Madison\", \"Alexander Hamilton\"]:\n",
    "    pyplot.figure(figsize=(14, 8))\n",
    "    pyplot.title(author)\n",
    "    pyplot.yticks([])\n",
    "    pyplot.xticks(range(75), top_words[:75], rotation=\"vertical\")\n",
    "    pyplot.imshow(doc_word_zscores[author_list == author,:75], vmin=-4, vmax=4)\n",
    "    pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part : Parts of Speech\n",
    "\n",
    "In addition to the `text` of each token, Spacy guesses the token's part of speech (POS), and stores the code for this POS in the variable `pos_`. Do the same Burrows' Delta analysis that we have done in the previous sections for part of speech. Do you see differences in use between Hamilton and Madison that are consistent enough that you could use them to identify the author of an unknown work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Written answer below]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part : Publications\n",
    "\n",
    "We always want to make sure when doing quantitative analysis that we aren't just making stuff up. The measurements will always produce numbers, and we're really good at seeing patterns -- even when they don't exist.\n",
    "\n",
    "What happens when we do the same type of analysis but on a variable that we *don't* necessarily expect to impact word use? Do the same Burrows' Delta analysis on words (not POS) that we have done in the previous sections, but instead of the `\"Author\"` field use the `\"Publication\"` field. Compare the two main publication venues, the New York Packet and the Independent Journal. Make an argument based on differences you do or do not observe whether these two publications are statistically distinguishable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Written answer below]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

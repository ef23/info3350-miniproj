{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theory: Entropy and Kullback-Leibler Divergence\n",
    "\n",
    "Today we will go over two functions from information theory and practice working with numeric arrays in `numpy`.\n",
    "\n",
    "The *entropy* of a probability distribution is $-\\sum_i p_i \\log p_i$, where $p_i$ is the probability of value $i$ under distribution $p$.\n",
    "\n",
    "The *KL divergence* between two probability distributions is $\\sum_i p_i \\log \\frac{p_i}{q_i}$.\n",
    "\n",
    "We need to be careful in implementing these functions. They include log and division, so we need to check for zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Wednesday we will start using the `spacy` library to help with tokenization and sentence splitting. Make sure you have it installed and running by then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure this works for Wednesday\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some useful numpy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a numeric array from a regular python array\n",
    "x = numpy.array([0.2, 0.5, 0.3])\n",
    "\n",
    "x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty array and fill specific values\n",
    "y = numpy.zeros(5)\n",
    "\n",
    "y[1] = 0.3\n",
    "y[3] = 0.7\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the array positions (indices) of non-zero values\n",
    "bit_array = numpy.array([0, 1, 1, 0, 0, 1, 0, 1])\n",
    "numpy.nonzero(bit_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select multiple values from an array by asking for an array of integers\n",
    "bit_array[ numpy.nonzero(bit_array) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log base 2\n",
    "numpy.log2(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can apply numpy functions to every element of an array with one expression\n",
    "numpy.log2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "\n",
    "Write a function `entropy` that takes a numeric array `p` and returns its entropy: $-\\sum_i p_i \\log_2 p_i$. Decide what to do if `p` does not sum to 1.0, implement this decision, and state what happens in a comment. Ignore values of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\n",
    "\n",
    "Apply your entropy function to at least four distributions with three elements. What is the largest value you can get, and what is the smallest? What is the significance of the largest value? Does it matter which order the values appear in the distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Written answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\n",
    "\n",
    "Now write a function `kl` that takes two arrays `p` and `q`, and returns the KL divergence: $\\sum_i p_i \\log \\frac{p_i}{q_i}$. Check that both arrays sum to 1.0, and make a decision about what you will do if they do not. Record and implement that decision, as before. Ignore elements for which `p` is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\n",
    "\n",
    "Try several combinations of probability distributions. What are the largest and smallest values you can reach? Does it matter if you switch `p` and `q`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Written answer here]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\n",
    "\n",
    "Jensen-Shannon distance is a popular alternative to KL. For distributions `p` and `q` we create a third distribution `r` such that $r_i = \\frac{p_i + q_i}{2}$. The JS distance is then half the sum of $KL(p, r)$ and $KL(q, r)$.\n",
    "\n",
    "Write a function `js` that takes two probability distributions `p` and `q` and returns the JS distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\n",
    "\n",
    "Try several combinations of probability distributions. How is JS different from KL? Under what circumstances would we prefer JS over KL, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Written answer here]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

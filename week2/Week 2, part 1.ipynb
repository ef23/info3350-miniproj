{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2, part 1: tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'the', 'last', 'five', 'years', 'dozens', 'of', 'schools', 'have', 'popped', 'up', 'offering', 'an', 'unusual', 'promise', 'Even', 'humanities', 'graduates', 'can', 'learn', 'how', 'to', 'code', 'in', 'a', 'few', 'months', 'and', 'join', 'the', 'high', 'paying', 'digital', 'economy', 'Students', 'and', 'their', 'hopeful', 'parents', 'shelled', 'out', 'as', 'much', 'as', '26', '000', 'seeking', 'to', 'jump', 'start', 'a', 'career']\n",
      "[]\n",
      "['But', 'the', 'coding', 'boot', 'camp', 'field', 'now', 'faces', 'a', 'sobering', 'moment', 'as', 'two', 'large', 'schools', 'have', 'announced', 'plans', 'to', 'shut', 'down', 'this', 'year', 'despite', 'backing', 'by', 'major', 'for', 'profit', 'education', 'companies', 'Kaplan', 'and', 'the', 'Apollo', 'Education', 'Group', 'the', 'parent', 'of', 'the', 'University', 'of', 'Phoenix']\n",
      "[]\n",
      "['The', 'closings', 'are', 'a', 'sign', 'that', 'years', 'of', 'heady', 'growth', 'led', 'to', 'a', 'boot', 'camp', 'glut', 'and', 'that', 'the', 'field', 'could', 'be', 'in', 'the', 'early', 'stages', 'of', 'a', 'shakeout']\n",
      "[]\n",
      "['You', 'can', 'imagine', 'this', 'becoming', 'a', 'big', 'industry', 'but', 'not', 'for', '90', 'companies', 'said', 'Michael', 'Horn', 'a', 'principal', 'consultant', 'at', 'Entangled', 'Solutions', 'an', 'education', 'research', 'and', 'consulting', 'firm']\n"
     ]
    }
   ],
   "source": [
    "### On Monday we talked about how encodings allow us to map\n",
    "###  bytes to characters. In this lesson we'll look at finding\n",
    "###  meaningful groups of characters.\n",
    "\n",
    "### In the git repo you will find six files containing \n",
    "###  sample text. Each is selected to highlight issues in\n",
    "###  tokenization. I've included a template for the first\n",
    "###  one, which we will modify together in class.\n",
    "\n",
    "import re, sys\n",
    "\n",
    "## SAMPLE 1\n",
    "\n",
    "## Here's an example of a simple pattern defining a word token. \n",
    "word_pattern = re.compile(\"[\\w]+\")\n",
    "\n",
    "## Specify the UTF-8 encoding. This is default on Mac/Linux, but not all Windows.\n",
    "with open(\"sample1.txt\", encoding=\"utf-8\") as file:\n",
    "    \n",
    "    ## This block reads a file line by line.\n",
    "    for line in file:\n",
    "        line = line.rstrip()\n",
    "        \n",
    "        ## This converts a string (line) into a list (tokens)\n",
    "        tokens = word_pattern.findall(line)\n",
    "        \n",
    "        print(tokens)\n",
    "\n",
    "### [Discuss sample 1 here.]\n",
    "\n",
    "## SAMPLE 2\n",
    "\n",
    "### [Copy and modify code as necessary here.]\n",
    "\n",
    "### [Discuss sample 2 here.]\n",
    "\n",
    "\n",
    "## SAMPLE 3\n",
    "\n",
    "### [Copy and modify code as necessary here.]\n",
    "\n",
    "### [Discuss sample 3 here.]\n",
    "\n",
    "\n",
    "## SAMPLE 4\n",
    "\n",
    "### [Copy and modify code as necessary here.]\n",
    "\n",
    "### [Discuss sample 4 here.]\n",
    "\n",
    "\n",
    "## SAMPLE 5\n",
    "\n",
    "### [Copy and modify code as necessary here.]\n",
    "\n",
    "### [Discuss sample 5 here.]\n",
    "\n",
    "\n",
    "## SAMPLE 6\n",
    "\n",
    "### [Copy and modify code as necessary here.]\n",
    "\n",
    "### [Discuss sample 6 here.]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

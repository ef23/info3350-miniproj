{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2, part 2: Counting words and sentiment\n",
    "\n",
    "The two readings for this week try to model plot as a cycle of positivity \n",
    "and negativity. How well can we measure these constructs in novels?\n",
    "\n",
    "In this work we'll look at evaluating documents with respect to a fixed\n",
    "vocabulary. I've included two sample sentiment lexicons, one by \n",
    "Bing Liu and one from Matt Jockers' \"syuzhet\" package.\n",
    "\n",
    "( For an interactive look, see this page: https://mimno.infosci.cornell.edu/sentiment/ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, sys\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot\n",
    "import numpy, pandas\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. There's a bug in the code. All the paragraphs are being scored as 0.\n",
    "  Fix it, and describe what was happening. \n",
    "\n",
    " [Description here]\n",
    "\n",
    " 2. I'm using the Counter class from the `collections` package instead of a \n",
    "  python `dict`. Consult the documentation https://docs.python.org/3/library/collections.html\n",
    "  and describe four features that Counter provides that dict does not.\n",
    "\n",
    " a.\n",
    " b.\n",
    " c.\n",
    " d.\n",
    "\n",
    " 3. The directory `txt` contains works by Charles Dickens in the correct format:\n",
    "  one paragraph per line. Apply the two lexicons to `christmas.txt`. Do they work?\n",
    "  Do they agree? Provide specific examples.\n",
    "\n",
    " [Description here]\n",
    "\n",
    " 4. The code is currently just adding up all the scores for each word token.\n",
    "  This favors longer documents: if we just repeat the contents twice, the score doubles.\n",
    "  What happens if we normalize by document length? In the `score_counts` function,\n",
    "  divide the score by the total number of tokens.\n",
    "\n",
    " [Describe how the output changes here. Is this normalization a good idea? Why or why not?]\n",
    "\n",
    " 5. Working with your table, create a lexicon for one of the emotions listed on\n",
    "  on the board, or choose your own. You may collect additional documents to test\n",
    "  your lexicon, please include these if so.\n",
    "\n",
    " [Include your team's final lexicon in your zip file. Discuss here your personal experience. What was hard about this process?]\n",
    "\n",
    " 6. I've set this up so that we are looking at the most extreme passages in \n",
    "  the sources. What does this approach show, and what does it hide? How does it\n",
    "  affect how we evaluate the tool?\n",
    "\n",
    " [Discuss here]\n",
    "\n",
    " IDEAS FOR YOUR WRITING RESPONSE FOR NEXT FRIDAY:\n",
    "\n",
    " Both groups are working from Vonnegut's description of plot. Does this\n",
    "  view really reflect plot? If not, what is missing, and how important is it\n",
    "  to you?\n",
    "\n",
    " Given your experience with lexicon-based sentiment analysis, how well does\n",
    "  it approximate a quantity that's relevant for plot analysis?\n",
    "\n",
    " Would a more nuanced view of emotion lead to a better representation of plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_word_weights(lexicon_file):\n",
    "\n",
    "    ## Create a mapping from words to numbers\n",
    "    word_weights = {}\n",
    "    with open(lexicon_file) as lexicon_reader:\n",
    "        for line in lexicon_reader:\n",
    "            weight, word = line.split(\",\") ## split on comma\n",
    "            word_weights[word] = float(weight) ## convert string to number\n",
    "    \n",
    "    return word_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_weights = load_word_weights(\"bingliu.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_weights[\"miserable\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_weights[\"glory\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This function applies the word weights to a list of word counts\n",
    "def score_counts(counter, word_weights):\n",
    "    ## accumulate word weights in this variable\n",
    "    score = 0\n",
    "    \n",
    "    ## count the words in the passage\n",
    "    total_tokens = sum(counter.values())\n",
    "    ## check for empty segments\n",
    "    if total_tokens == 0:\n",
    "        return 0\n",
    "    \n",
    "    ## for each word, look up its score\n",
    "    for word in counter.keys():\n",
    "        if word in word_weights:\n",
    "            score += word_weights[word] * counter[word]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_counter = Counter([\"happy\", \"happy\", \"joy\", \"joy\"])\n",
    "\n",
    "score_counts(sample_counter, word_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_counts(Counter([\"happy\", \"happy\", \"joy\", \"joy\", \"despair\"]), word_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_paragraphs(text_file):\n",
    "\n",
    "    ## Here's an example of a simple pattern defining a word token. \n",
    "    word_pattern = re.compile(\"\\w[\\w\\-\\']*\\w|\\w\") ## what matches this?\n",
    "\n",
    "    ## Now look at the actual documents. We'll create a list with one object per text segment.\n",
    "    paragraphs = []\n",
    "    \n",
    "    ## here's where we actually read the file\n",
    "    with open(text_file, encoding=\"utf-8\") as file:\n",
    "    \n",
    "        ## This block reads a file line by line.\n",
    "        for line in file:\n",
    "            line = line.rstrip()\n",
    "        \n",
    "            tokens = word_pattern.findall(line)\n",
    "        \n",
    "            ## turn a list into a word->count map\n",
    "            paragraph_counts = Counter(tokens)\n",
    "        \n",
    "            ## create the paragraph object, with the original text, \n",
    "            ##  the word counts, and the total score.\n",
    "            paragraphs.append({'text': line, 'counts': paragraph_counts,\n",
    "                               'score': score_counts(paragraph_counts, word_weights) })\n",
    "\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paragraphs = load_paragraphs(\"txt/christmas.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show the contents of the data structure for a paragraph\n",
    "\n",
    "paragraphs[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_paragraphs = sorted(paragraphs, key=lambda x: x[\"score\"])\n",
    "\n",
    "## Display the 10 most negative\n",
    "for paragraph in sorted_paragraphs[0:9]:\n",
    "    print(\"{}\\t{}\".format(paragraph['score'], paragraph['text']))\n",
    "\n",
    "## ... and the 10 most positive\n",
    "for paragraph in sorted_paragraphs[-10:-1]:\n",
    "    print(\"{}\\t{}\".format(paragraph['score'], paragraph['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores = numpy.array([p[\"score\"] for p in paragraphs])\n",
    "\n",
    "pyplot.figure(figsize=(20, 5))\n",
    "pyplot.plot(sentiment_scores)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_mean = pandas.Series(sentiment_scores).rolling(20).mean()\n",
    "\n",
    "pyplot.figure(figsize=(20, 5))\n",
    "pyplot.plot(rolling_mean)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in paragraphs[600:630]:\n",
    "    print(f\"{p['score']:.2f}\\t{p['text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
